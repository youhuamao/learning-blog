{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "480ec5e6-3efc-4dd3-ae9c-16f7f32275a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "# @Author   : huhu\n",
    "# @Time     : 2023/3/20 9:14\n",
    "# @File     : train.py.py\n",
    "# @Project  : blog_04\n",
    "# @objective: \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e2bcf6-5b68-4688-96d7-813a5114687f",
   "metadata": {},
   "source": [
    "#### 加载数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32fd6986-28be-4049-b263-35b62ae72d6e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      news_world\n",
       "1    news_finance\n",
       "2        news_edu\n",
       "3     news_sports\n",
       "4     news_sports\n",
       "Name: 分类名称, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 得到评论，normal_file为存放正常评论的文件，spam_file为存放垃圾评论的文件\n",
    "train_data = pd.read_csv('data/train/train.csv')\n",
    "test_data  = pd.read_csv('data/test/test.csv')\n",
    "\n",
    "# print (train_data.head(2))\n",
    "\n",
    "# 将特征划分到 X 中，标签划分到 Y 中\n",
    "x = train_data.iloc[:, 1:]\n",
    "y = train_data.iloc[:, 0]\n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c23ee93d-b39d-4d44-8970-22dd30ba877f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "effcb644-179c-497e-b903-bb84282b1f70",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['科学家警告：“害虫杂交群”威胁全球农作物',\n",
       " '企业“走出去”有了更多公共服务',\n",
       " '大专毕业多年，2018年入学成人本科，本科毕业后，能否参加司法考试？',\n",
       " '历史第一！C罗将迎来欧冠金靴六连霸！',\n",
       " '今日二串：世界杯之战，就是本彩店决定胜负，因为他看的出结果']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x['新闻字符串'].to_list()[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7798e806-89b5-4bb1-a3a9-89dd399f9b98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "y_dict_temp = list(set(y.to_list()))\n",
    "y_dict_temp.sort()\n",
    "# y_dict = [[y_dict_temp[index], index] for index in range(len(y_dict_temp))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1803fd4d-c3b3-42df-b352-c1b186efc04c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['news_agriculture',\n",
       " 'news_car',\n",
       " 'news_culture',\n",
       " 'news_edu',\n",
       " 'news_entertainment',\n",
       " 'news_finance',\n",
       " 'news_game',\n",
       " 'news_house',\n",
       " 'news_military',\n",
       " 'news_sports',\n",
       " 'news_story',\n",
       " 'news_tech',\n",
       " 'news_travel',\n",
       " 'news_world',\n",
       " 'stock']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4338a2b2-d9ed-40da-8407-eb3b3601cfe2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "range(0, 15)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range(len(y_dict_temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb35282f-f4a4-405e-82c3-bbd4f2a57f8e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'news_agriculture': 0,\n",
       " 'news_car': 1,\n",
       " 'news_culture': 2,\n",
       " 'news_edu': 3,\n",
       " 'news_entertainment': 4,\n",
       " 'news_finance': 5,\n",
       " 'news_game': 6,\n",
       " 'news_house': 7,\n",
       " 'news_military': 8,\n",
       " 'news_sports': 9,\n",
       " 'news_story': 10,\n",
       " 'news_tech': 11,\n",
       " 'news_travel': 12,\n",
       " 'news_world': 13,\n",
       " 'stock': 14}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 生成字典\n",
    "y_dict = dict([y_dict_temp[index], index] for index in range(len(y_dict_temp)))\n",
    "\n",
    "y_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6fb04861-a8fa-4844-a5f1-66c62528478e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 字典的键值反转\n",
    "y_dict_temp = dict(zip(y_dict.values(), y_dict.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "973fb490-03f7-4aac-8469-85770da3944e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'news_agriculture',\n",
       " 1: 'news_car',\n",
       " 2: 'news_culture',\n",
       " 3: 'news_edu',\n",
       " 4: 'news_entertainment',\n",
       " 5: 'news_finance',\n",
       " 6: 'news_game',\n",
       " 7: 'news_house',\n",
       " 8: 'news_military',\n",
       " 9: 'news_sports',\n",
       " 10: 'news_story',\n",
       " 11: 'news_tech',\n",
       " 12: 'news_travel',\n",
       " 13: 'news_world',\n",
       " 14: 'stock'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict_temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "19b7307d-0d62-419e-8cb4-bf8bf31630f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 使用数值标签替换文字标签\n",
    "y = [y_dict[index] for index in y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2b773454-93af-4bbe-ad10-b49dfe1349b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e13a97-0864-47e1-bd2e-6d90a16b06ed",
   "metadata": {},
   "source": [
    "### 构建、训练模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c186de11-12cb-4675-b929-474c4ee83ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Dense, Embedding, Flatten, Dropout, Conv1D, GlobalMaxPooling1D\n",
    "from tensorflow.keras.models import Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15465d35-fecd-4147-a0d8-f75724c04b7f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2403 unique tokens.\n"
     ]
    }
   ],
   "source": [
    "# 加载数据集，data 和 labels 分别为训练样本和标签\n",
    "data = x['新闻字符串'].to_list()\n",
    "labels = y\n",
    "\n",
    "# 对文本数据进行分词处理\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "# 文本标记实用类。该类允许使用两种方法向量化一个文本语料库： 将每个文本转化为一个整数序列（每个整数都是词典中标记的索引）； \n",
    "# 或者将其转化为一个向量，其中每个标记的系数可以是二进制值、词频、TF-IDF权重等。\n",
    "# num_words: 需要保留的最大词数，基于词频\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(data)\n",
    "sequences = tokenizer.texts_to_sequences(data)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "# 这个函数将num_samples个文本序列列表 (每个序列为整数列表) 转换成一个 2D Numpy数组，数组形状为 (num_samples, num_timesteps)\n",
    "# 如果指定了参数 maxlen 的值，则num_timesteps的值取maxlen的值，否则num_timesteps的值等于最长序列的长度。\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "# 将标签进行 one-hot 编码\n",
    "labels = tf.keras.utils.to_categorical(labels)\n",
    "\n",
    "# 划分训练集和测试集\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd79dbf8-b1a1-42e9-bb39-069fbee5f2b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bfa81624-fa3c-4161-aa06-f8c314975a4f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 1000, 1000)        10000000  \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 1000, 32)          96032     \n",
      "                                                                 \n",
      " global_max_pooling1d (Globa  (None, 32)               0         \n",
      " lMaxPooling1D)                                                  \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               8448      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 15)                975       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,146,607\n",
      "Trainable params: 10,146,607\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 构建模型\n",
    "model = Sequential()\n",
    "\n",
    "embedding_dim = 1000\n",
    "\n",
    "model.add(Embedding(input_dim=MAX_NB_WORDS,\n",
    "                    output_dim=embedding_dim,\n",
    "                    input_length=MAX_SEQUENCE_LENGTH))\n",
    "\n",
    "model.add(Conv1D(filters=32, kernel_size=3, padding='same', activation='relu'))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(256, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(128, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(labels.shape[1], activation='softmax'))\n",
    "\n",
    "model.summary()  # 打印模型的结构和参数列表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "96528c3c-3c0b-4f0f-8131-b9369c1fe5c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 编译模型\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c9579080-0ecc-4c9a-9ac3-6a22b661f9b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "7/7 [==============================] - 26s 3s/step - loss: 2.6876 - accuracy: 0.1194 - val_loss: 2.6430 - val_accuracy: 0.1250\n",
      "Epoch 2/5\n",
      "7/7 [==============================] - 24s 3s/step - loss: 2.6257 - accuracy: 0.1187 - val_loss: 2.6410 - val_accuracy: 0.0850\n",
      "Epoch 3/5\n",
      "7/7 [==============================] - 22s 3s/step - loss: 2.6078 - accuracy: 0.1150 - val_loss: 2.6415 - val_accuracy: 0.0850\n",
      "Epoch 4/5\n",
      "7/7 [==============================] - 23s 3s/step - loss: 2.5940 - accuracy: 0.1244 - val_loss: 2.6271 - val_accuracy: 0.1250\n",
      "Epoch 5/5\n",
      "7/7 [==============================] - 23s 3s/step - loss: 2.5611 - accuracy: 0.1375 - val_loss: 2.6163 - val_accuracy: 0.1250\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a9f52bd450>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练模型\n",
    "model.fit(X_train, y_train, batch_size=256, epochs=5, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5910d619-73a9-40f9-9481-4b1b65560092",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 模型存储\n",
    "save_path = 'model/test.h5'\n",
    "\n",
    "model.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "822c6a66-ef95-4312-bfd1-1bb49322cc05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "from keras.models import load_model\n",
    "save_path = 'model/test.h5'\n",
    "model_test = load_model(save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14a269f-6092-4331-bca1-f9cbd7d32fd1",
   "metadata": {},
   "source": [
    "#### 预测数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c17f0b44-8ffc-4073-b609-a5a88d0bdc74",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['科学家警告：“害虫杂交群”威胁全球农作物']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_data = [x['新闻字符串'].to_list()[0]]\n",
    "pred_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "951c2d18-a598-4498-b21e-21e18164e5c1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 对文本数据进行分词处理\n",
    "MAX_NB_WORDS = 10000\n",
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "sequences = tokenizer.texts_to_sequences(pred_data)\n",
    "pred_data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9195fe8-6cab-41ac-ace7-e1eff47964e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 145ms/step\n"
     ]
    }
   ],
   "source": [
    "ynew = model_test.predict(pred_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "82d9e2ed-85db-4c75-aa36-d63841bf478c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.04992791, 0.11276272, 0.0689282 , 0.06253488, 0.10451338,\n",
       "       0.07689668, 0.07057061, 0.04254714, 0.0592688 , 0.08783774,\n",
       "       0.02688343, 0.11169963, 0.05471561, 0.05030889, 0.02060437],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ynew[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7fcef931-307b-426a-9765-0c95a0d05ad3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy.argmax(array, axis) 用于返回一个numpy数组中最大值的索引值。\n",
    "# 当一组中同时出现几个最大值时，返回第一个最大值的索引值。\n",
    "y_test_pred = np.argmax(ynew, axis=1)\n",
    "y_test_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d2d6d8c5-1a1b-461e-9b96-5a6cafaac6d3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'news_car'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_dict_temp[y_test_pred[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50505b20-6bf5-4493-ac42-e159e8968517",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e7c0d63-82c6-4b71-93fe-da1699d78393",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
